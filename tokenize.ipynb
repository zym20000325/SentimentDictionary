{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clear(df):\n",
    "    \n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # 增加一列 words\n",
    "    df['words'] = ''   \n",
    "    \n",
    "\n",
    "    pattern1='[a-zA-Z0-9]'\n",
    "    pattern2 = '\\[.*?\\]'\n",
    "    pattern3 = re.compile(u'[^\\s1234567890:：' + '\\u4e00-\\u9fa5]+')\n",
    "    pattern4='[’!\"#$%&\\'()*+,-./：:;<=>?@[\\\\]^_`{|}~]+'\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        # 去空格\n",
    "        df['words'][i] = df['content'][i].replace(' ','')\n",
    "\n",
    "        line1=re.sub(pattern1,'',df['words'][i])   #去除字母和数字\n",
    "        line2=re.sub(pattern2,'',line1)            #去除表情\n",
    "        line3=re.sub(pattern3,'',line2)            #去除其它字符\n",
    "        line4=re.sub(pattern4, '', line3)          #去掉残留的冒号及其它符号\n",
    "        new_text=''.join(line4.split())       \n",
    "        \n",
    "        df['words'][i] = new_text\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langconv import *\n",
    "\n",
    "# 将繁体转换成简体\n",
    "    \n",
    "def tradition2simple(line):\n",
    "    \n",
    "    line = Converter('zh-hans').convert(line)\n",
    "#     line = line.encode('utf-8')\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.624 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Paddle enabled successfully......\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import paddle\n",
    "paddle.enable_static()\n",
    "\n",
    "jieba.load_userdict(\"data/dictionary/user_dict.txt\")     # 加入自定义金融词典\n",
    "jieba.enable_paddle()    \n",
    "\n",
    "# 去停词\n",
    "def stopwordslist(filepath):  \n",
    "    \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords  \n",
    " \n",
    "def seg_sentence(sentence,use_paddle=False):  \n",
    "    \n",
    "    sentence = tradition2simple(sentence)\n",
    "    \n",
    "    sentence_seged = jieba.cut(sentence, use_paddle=use_paddle)  \n",
    "    stopwords = stopwordslist('data/stopwords/user_stopwords.txt')  # 停用词的路径  \n",
    "    outstr = ''  \n",
    "    for word in sentence_seged:  \n",
    "        if word not in stopwords:  \n",
    "            if word != '\\t':  \n",
    "                outstr += word  \n",
    "                outstr += \"/\"   \n",
    "    return outstr\n",
    "\n",
    "def token(df,user_paddle=False):\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        df.words[i] = seg_sentence(df.words[i])                    # 选择精确模式\n",
    "#         df.words[i] = seg_sentence(df.words[i],use_paddle=True)    # 选择paddle模式\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_file(filename,t1,t2):\n",
    "    \n",
    "    df_hs300 = pd.read_csv('data/HS300/' + filename + '.csv')\n",
    "    stock_code = list(df_hs300['成份代码'])\n",
    "    foldername = t1+'-'+t2\n",
    "    \n",
    "    for i in range(len(stock_code)):\n",
    "        \n",
    "        one_stock_code = stock_code[i]\n",
    "        savefilepath = 'data/HS300/' + foldername + '/'+ one_stock_code+'.csv'\n",
    "        df = pd.read_csv(savefilepath)\n",
    "        df = clear(df)\n",
    "        df = token(df)\n",
    "        df.to_csv(savefilepath, index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_file('2015-12-30','20151230','20160613')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_file('2016-06-13','20160613','20161212')\n",
    "# token_file('2016-12-12','20161212','20170612')\n",
    "\n",
    "# token_file('2017-06-12','20170612','20171211')\n",
    "# token_file('2017-12-11','20171211','20180611')\n",
    "\n",
    "# token_file('2018-06-11','20180611','20181217')\n",
    "# token_file('2018-12-17' ,'20181217','20190617')\n",
    "\n",
    "# token_file('2019-06-17','20190617','20191216')\n",
    "# token_file('2019-12-16','20191216','20200615')\n",
    "\n",
    "token_file('2020-06-15','20200615','20201214')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/HS300/20151230-20160613/000009.SZ.csv\")\n",
    "df = clear(df)\n",
    "df = token(df)\n",
    "df.to_csv(savefilepath, index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.words[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.words[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.words[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
